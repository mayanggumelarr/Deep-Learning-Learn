{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-18T01:54:01.706709Z",
     "start_time": "2025-09-18T01:54:01.701474Z"
    }
   },
   "source": [
    "from string import punctuation\n",
    "\n",
    "teks = \"Nama Saya Mayang Gumelar\"\n",
    "\n",
    "teks_lower = teks.lower()\n",
    "teks_upper = teks.upper()\n",
    "\n",
    "print(teks)\n",
    "print(teks_lower)\n",
    "print(teks_upper)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nama Saya Mayang Gumelar\n",
      "nama saya mayang gumelar\n",
      "NAMA SAYA MAYANG GUMELAR\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T01:59:43.487417Z",
     "start_time": "2025-09-18T01:59:43.479750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# HAPUS ANGKA\n",
    "\n",
    "def hapus_angka(teks):\n",
    "    teks_tanpa_angka = ''.join([char for char in teks if not char.isdigit()]) #isdigit adalah fungsi memastikan itu angka\n",
    "    return  teks_tanpa_angka\n",
    "tekss = \"Aku suka 1234 yakan\"\n",
    "teks_tnp_angka = hapus_angka(tekss)\n",
    "\n",
    "print(tekss)\n",
    "print(teks_tnp_angka)"
   ],
   "id": "54d582ae4c6d83d0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aku suka 1234 yakan\n",
      "Aku suka  yakan\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T02:13:10.857604Z",
     "start_time": "2025-09-18T02:13:10.839193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Delete Tanda baca\n",
    "import string\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    punctuation_set = set(string.punctuation)\n",
    "\n",
    "    text_without_punctuation = ''.join(char for char in text if char not in punctuation_set)\n",
    "\n",
    "    return text_without_punctuation\n",
    "\n",
    "cth_teks = \"Ini adalah teks? untuk contoh! ya@\"\n",
    "cth_cleaning_teks = remove_punctuation(cth_teks)"
   ],
   "id": "6ca61da465147e0a",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T02:13:37.709952Z",
     "start_time": "2025-09-18T02:13:37.703341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(cth_teks)\n",
    "print(cth_cleaning_teks)"
   ],
   "id": "ce4a4697c06c6126",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ini adalah teks? untuk contoh! ya@\n",
      "Ini adalah teks untuk contoh ya\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T02:15:37.912190Z",
     "start_time": "2025-09-18T02:15:37.908296Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Delete whitespace\n",
    "\n",
    "teks = \" Ini adalah teks    dg whitespace   h\"\n",
    "teks_clean_whitespace = teks.strip()\n",
    "print(teks_clean_whitespace)"
   ],
   "id": "4c46d0afef1008ed",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ini adalah teks    dg whitespace   h\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T02:24:32.444026Z",
     "start_time": "2025-09-18T02:24:18.663628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# HAPUS KATA TIDAK PENTING DENGAN STOPWORDS\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download korpus stopwords bahasa Indonesia dari NLTK jika belum terunduh\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')  # Untuk tokenisasi kata\n",
    "\n",
    "teks = \"Perekonomian Indonesia sedang dalam pertumbuhan yang membanggakan.\"\n",
    "\n",
    "# Tokenisasi teks menjadi kata-kataD:\\python\n",
    "tokens_kata = word_tokenize(teks)\n",
    "\n",
    "# Ambil daftar stopwords bahasa Indonesia dari NLTK\n",
    "stopwords_indonesia = set(stopwords.words('indonesian'))\n",
    "\n",
    "# Filtering kata-kata dengan menghapus stopwords\n",
    "kata_penting = [kata for kata in tokens_kata if kata.lower() not in stopwords_indonesia]\n",
    "\n",
    "# Gabungkan kata-kata penting kembali menjadi teks\n",
    "teks_tanpa_stopwords = ' '.join(kata_penting)\n",
    "\n",
    "print(\"Teks asli:\", teks)\n",
    "print(\"Teks setelah filtering stopwords NLTK:\", teks_tanpa_stopwords)"
   ],
   "id": "66c02dc9f896bee4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teks asli: Perekonomian Indonesia sedang dalam pertumbuhan yang membanggakan.\n",
      "Teks setelah filtering stopwords NLTK: Perekonomian Indonesia pertumbuhan membanggakan .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T02:31:29.583340Z",
     "start_time": "2025-09-18T02:31:29.566581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# WITH SASTRAWI\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Inisialisasi objek StopWordRemover dari Sastrawi\n",
    "factory = StopWordRemoverFactory()\n",
    "stopwords_sastrawi = factory.get_stop_words()\n",
    "\n",
    "teks = \"Perekonomian Indonesia sedang dalam pertumbuhan yang membanggakan.\"\n",
    "\n",
    "# Tokenisasi teks menjadi kata-kata\n",
    "tokens_kata = word_tokenize(teks)\n",
    "\n",
    "# Filtering kata-kata dengan menghapus stopwords Sastrawi\n",
    "kata_penting = [kata for kata in tokens_kata if kata.lower() not in stopwords_sastrawi]\n",
    "\n",
    "# Gabungkan kata-kata penting kembali menjadi teks\n",
    "teks_tanpa_stopwords = ' '.join(kata_penting)\n",
    "\n",
    "print(\"Teks asli:\", teks)\n",
    "print(\"Teks setelah filtering stopwords Sastrawi:\", teks_tanpa_stopwords)"
   ],
   "id": "10d76a16ee33a9d8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teks asli: Perekonomian Indonesia sedang dalam pertumbuhan yang membanggakan.\n",
      "Teks setelah filtering stopwords Sastrawi: Perekonomian Indonesia sedang pertumbuhan membanggakan .\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T03:33:54.147275Z",
     "start_time": "2025-09-18T03:33:54.121202Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TOKENIZING\n",
    "# Proses membagi teks ke potongan2 yang lebih kecil\n",
    "\n",
    "# TOKENISASI KATA\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Ini adalah contoh tokenisasi kata dalam pemrosesan teks.\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ],
   "id": "2b96900746f2d66c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ini', 'adalah', 'contoh', 'tokenisasi', 'kata', 'dalam', 'pemrosesan', 'teks', '.']\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T03:34:13.744231Z",
     "start_time": "2025-09-18T03:34:13.725257Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TOKENISASI KALIMAT\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"Ini adalah contoh tokenisasi kalimat. Apakah ini kalimat kedua? Ya, ini kalimat ketiga!\"\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences)"
   ],
   "id": "b574bc221b95c0cf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ini adalah contoh tokenisasi kalimat.', 'Apakah ini kalimat kedua?', 'Ya, ini kalimat ketiga!']\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T03:34:47.132104Z",
     "start_time": "2025-09-18T03:34:47.123011Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TOKENISASI FRASA\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "# Misalkan kita ingin memisahkan frasa berdasarkan tanda baca koma (,)\n",
    "text = \"Pemrosesan teks adalah cabang ilmu komputer yang berfokus pada pengolahan teks dan dokumen.\"\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "phrases = tokenizer.tokenize(text)\n",
    "print(phrases)"
   ],
   "id": "26e6750b1e7728a2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pemrosesan', 'teks', 'adalah', 'cabang', 'ilmu', 'komputer', 'yang', 'berfokus', 'pada', 'pengolahan', 'teks', 'dan', 'dokumen', '.']\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T03:37:29.559674Z",
     "start_time": "2025-09-18T03:37:29.544744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# STEMMING\n",
    "# PROSES PENYEDERHANAAN KATA\n",
    "# Stemming hanya sebatas pada pemotongan imbuhan dan akhiran\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Inisialisasi stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Kata-kata asli\n",
    "words = [\"running\", \"runs\", \"runner\", \"ran\", \"easily\", \"fairness\", \"better\", \"best\", \"cats\", \"cacti\", \"geese\", \"rocks\", \"oxen\"]\n",
    "\n",
    "# Melakukan stemming pada setiap kata\n",
    "for word in words:\n",
    "    stemmed_word = stemmer.stem(word)\n",
    "    print(f\"Kata asli: {word}, stemming: {stemmed_word}\")"
   ],
   "id": "6b611dfca9131b28",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kata asli: running, stemming: run\n",
      "Kata asli: runs, stemming: run\n",
      "Kata asli: runner, stemming: runner\n",
      "Kata asli: ran, stemming: ran\n",
      "Kata asli: easily, stemming: easili\n",
      "Kata asli: fairness, stemming: fair\n",
      "Kata asli: better, stemming: better\n",
      "Kata asli: best, stemming: best\n",
      "Kata asli: cats, stemming: cat\n",
      "Kata asli: cacti, stemming: cacti\n",
      "Kata asli: geese, stemming: gees\n",
      "Kata asli: rocks, stemming: rock\n",
      "Kata asli: oxen, stemming: oxen\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T03:44:26.180788Z",
     "start_time": "2025-09-18T03:43:48.917388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# LEMMATISASI\n",
    "# Proses mendapatkan kata paling dasar dari sebuah kata yang telah mengalami perubahan\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download wordnet jika belum di-download\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Inisialisasi lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Kata-kata asli\n",
    "words = [\"Run\", \"Cat\", \"Good\", \"Goose\", \"Rock\", \"City\", \"Big\", \"Happy\", \"Run\", \"Sleep\"]\n",
    "\n",
    "# Melakukan lematisasi pada setiap kata\n",
    "for word in words:\n",
    "    lemma_word = lemmatizer.lemmatize(word.lower())  # Mengonversi ke huruf kecil untuk memastikan pemrosesan yang konsisten\n",
    "    print(f\"Kata asli: {word}, Kata setelah lematisasi: {lemma_word}\")\n"
   ],
   "id": "37acd85c002316bf",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kata asli: Run, Kata setelah lematisasi: run\n",
      "Kata asli: Cat, Kata setelah lematisasi: cat\n",
      "Kata asli: Good, Kata setelah lematisasi: good\n",
      "Kata asli: Goose, Kata setelah lematisasi: goose\n",
      "Kata asli: Rock, Kata setelah lematisasi: rock\n",
      "Kata asli: City, Kata setelah lematisasi: city\n",
      "Kata asli: Big, Kata setelah lematisasi: big\n",
      "Kata asli: Happy, Kata setelah lematisasi: happy\n",
      "Kata asli: Run, Kata setelah lematisasi: run\n",
      "Kata asli: Sleep, Kata setelah lematisasi: sleep\n"
     ]
    }
   ],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
